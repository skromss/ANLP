import sys
import os
from pathlib import Path

# Add project root to path
ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(ROOT))
sys.path.insert(0, str(Path(__file__).parent.parent))

# Fix encoding
sys.stdout.reconfigure(encoding='utf-8')

from src.graph import create_graph
from langchain_core.messages import HumanMessage

def run_query(graph, query: str):
    print(f"\n{'='*50}")
    print(f"User Query: {query}")
    print(f"{'='*50}")
    
    initial_state = {"messages": [HumanMessage(content=query)]}
    
    events = graph.stream(initial_state)
    
    for event in events:
        for key, value in event.items():
            print(f"\n--- Node: {key} ---")
            if "next" in value:
                print(f"Routing Decision: {value['next']}")
            if "messages" in value:
                last_msg = value["messages"][-1]
                if hasattr(last_msg, "tool_calls") and last_msg.tool_calls:
                    print(f"Tool Call: {last_msg.tool_calls[0]['name']}")
                    print(f"Args: {last_msg.tool_calls[0]['args']}")
                elif hasattr(last_msg, "content") and last_msg.content:
                    print(f"Response: {last_msg.content}")

def run_session(graph, queries):
    print(f"\n{'='*50}")
    print("Starting Interactive Session (Memory Demo)")
    print(f"{'='*50}")
    
    # Initialize state once
    state = {"messages": []}
    
    for query in queries:
        print(f"\nUser: {query}")
        
        # Append new user message
        state["messages"].append(HumanMessage(content=query))
        
        # Run graph with current state
        # LangGraph runs often return the final state, or we iterate events.
        # To maintain state, we need to extract the final messages.
        
        events = graph.stream(state)
        final_state = None
        
        for event in events:
            for key, value in event.items():
                if "messages" in value:
                    # Update (append) messages to our local state view for valid next turn
                    # Note: In a real app, the graph handles persistence. 
                    # Here we just manually carry over for the demo loop.
                    # LangGraph's stream output patches the state.
                    pass
                
                # Visual logging
                if "next" in value:
                   pass # print(f"  [Router]: {value['next']}")
                
                if "messages" in value:
                    last_msg = value["messages"][-1]
                    if hasattr(last_msg, "tool_calls") and last_msg.tool_calls:
                        print(f"  [Tool Call]: {last_msg.tool_calls[0]['name']}")
                    elif hasattr(last_msg, "content") and last_msg.content:
                        print(f"  [Agent]: {last_msg.content}")
            
            # Keep track of the final event chunk to update our state for the next loop
            # In 'stream', each event is a node update.
            # We can rely on the graph object if we used a checkpointer, but here we are stateless.
            # We need to manually capture the AI response to append to history.
            if "messages" in list(event.values())[0]:
                 final_state = list(event.values())[0]

        # Update our session state with the new messages generated by the graph
        # This is a bit simplified; dependent on how the graph returns deltas.
        # simpler approach: just take the last message from the last node and append it?
        # Better: Recurse or use invoke for state management if we want genuine state passing.
        
        # Let's switch to .invoke() for the session loop to get the full final state easily
        final_state_full = graph.invoke(state)
        state = final_state_full # Carry over entire state (history)

def main():
    graph = create_graph()
    
    # 1. Independent Queries (to show routing variety)
    print("--- PART 1: Independent Queries (Testing Routing) ---")
    queries_independent = [
        "What is the definition of a Major Scale?", 
        "I want to practice Guitar for 30 minutes.",
        "Calculate 5 * 10 (just kidding, tell me a joke instead)." 
    ]
    
    # Run them individually
    for q in queries_independent:
        run_query(graph, q)

    # 2. Conversational Session (Testing Memory)
    print("\n\n--- PART 2: Conversational Session (Testing Memory) ---")
    queries_session = [
        "Hi, I am Alice.",
        "What is my name?", # Tests memory
        "Tell me the notes in a C Major chord.",
        "Now give me a practice routine for Piano using that chord." # Contextual?
    ]
    
    run_session(graph, queries_session)

if __name__ == "__main__":
    main()
